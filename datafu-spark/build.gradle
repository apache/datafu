/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

// Much of this file is a variation on the Apache Samza build.gradle file

buildscript {
  repositories {
      mavenCentral()
  }
}

plugins {
  id "de.undercouch.download" version "3.4.3"
}

def scalaVersion = project.properties['scalaVersion'] ?: "2.12"

apply from: file("gradle/dependency-versions-scala-" + scalaVersion + ".gradle")

apply plugin: 'scala'

allprojects {
  // For all scala compilation, add extra compiler options, taken from version-specific
  // dependency-versions-scala file applied above.
  tasks.withType(ScalaCompile) {
    scalaCompileOptions.additionalParameters = [ scalaOptions ]
  }
}

// Note: There seem to be no differences between Spark versions 2.3 and 2.4 that require us to build separate
// versions.  If we do find differences in the future we can append '_' + sparkCompatVersion to the base name here.
archivesBaseName = 'datafu-spark_' + scalaVersion

cleanEclipse {
  doLast {
    delete ".apt_generated"
    delete ".settings"
    delete ".factorypath"
    delete "bin"
  }
}

dependencies {
    compile "org.apache.logging.log4j:log4j-api:$log4j2Version"
    compile "org.scala-lang:scala-library:$scalaLibraryVersion"
    compile ("org.apache.spark:spark-core_" + scalaVersion + ":" + sparkVersion)
    compile "org.apache.spark:spark-hive_" + scalaVersion + ":" + sparkVersion

    testCompile "org.apache.logging.log4j:log4j-1.2-api:$log4j2Version"
    testCompile "org.apache.logging.log4j:log4j-slf4j-impl:$log4j2Version"
    
// for versions that don't have a spark-test-base version use one that's older instead
    if (sparkVersion == "3.0.3") {
    	testCompile "com.holdenkarau:spark-testing-base_" + scalaVersion + ":3.0.2_" + sparkTestingBaseVersion
    } else if (sparkVersion == "3.1.3") {
    	testCompile "com.holdenkarau:spark-testing-base_" + scalaVersion + ":3.1.2_" + sparkTestingBaseVersion
    } else if (sparkVersion > "3.2" && sparkVersion < "3.3") {
    	testCompile "com.holdenkarau:spark-testing-base_" + scalaVersion + ":3.2.1_" + sparkTestingBaseVersion
    } else if (sparkVersion >= "3.3") {
    	testCompile "com.holdenkarau:spark-testing-base_" + scalaVersion + ":3.3.0_" + sparkTestingBaseVersion
    } else {
	testCompile "com.holdenkarau:spark-testing-base_" + scalaVersion + ":" + sparkVersion + "_" + sparkTestingBaseVersion
    }
}

project.ext.sparkFile = file("build/spark-zips/spark-" + sparkVersion + ".zip")
project.ext.sparkUnzipped = "build/spark-unzipped/spark-" + sparkVersion

// download pyspark for testing. This is not shipped with datafu-spark.
task downloadPySpark (type: Download) {
  src 'https://github.com/apache/spark/archive/v' + sparkVersion + '.zip'
  dest project.sparkFile
  onlyIfNewer true
}

downloadPySpark.onlyIf {
  ! project.sparkFile.exists()
}

task unzipPySpark(dependsOn: downloadPySpark, type: Copy) {
  from zipTree(downloadPySpark.dest)
  into file("build/spark-unzipped/")
}

unzipPySpark.onlyIf {
  ! file(project.sparkUnzipped).exists()
}

task zipPySpark(dependsOn: unzipPySpark, type: Zip) {
  archiveName = "pyspark-" + sparkVersion + ".zip"
  include "pyspark/**/*"
  destinationDir = file("data/pysparks/")
  from file(project.sparkUnzipped + "/python/")
}

zipPySpark.onlyIf {
  ! file("data/pysparks/pyspark-" + sparkVersion + ".zip").exists()
}

// download py4j for testing. This is not shipped with datafu-spark.
project.ext.py4js = [
  "py4j-0.10.8.1-src.zip" : "https://files.pythonhosted.org/packages/04/de/2d314a921ef4c20b283e1de94e0780273678caac901564df06b948e4ba9b/py4j-0.10.8.1-py2.py3-none-any.whl",
  "py4j-0.10.9-src.zip" : "https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl",
  "py4j-0.10.9.7-src.zip" : "https://files.pythonhosted.org/packages/10/30/a58b32568f1623aaad7db22aa9eafc4c6c194b429ff35bdc55ca2726da47/py4j-0.10.9.7-py2.py3-none-any.whl"
]

task downloadPy4js {
  doLast {
    for (s in py4js) {
      download {
        src s.value
        dest file("data/py4js/" + s.key)
      }
    }
  }
}

downloadPy4js.onlyIf {
  ! file("data/py4js").exists()
}


// The downloads of pyspark and py4j must succeed in order to test the Scala Python bridge in Eclipse or Gradle
tasks.eclipse.dependsOn('zipPySpark')
tasks.compileTestScala.dependsOn('zipPySpark')
tasks.eclipse.dependsOn('downloadPy4js')
tasks.compileTestScala.dependsOn('downloadPy4js')

test {
  systemProperty 'datafu.jar.dir', file('build/libs')
  systemProperty 'datafu.data.dir', file('data')

  systemProperty 'datafu.spark.version', sparkVersion

// removed so the daemon is cancelled
  maxHeapSize = "2G"
}

configurations.all {
    exclude group: 'log4j', module:'log4j'
    exclude group: 'log4j', module:'apache-log4j-extras'
    exclude group: 'org.slf4j', module:'slf4j-log4j12'
}
